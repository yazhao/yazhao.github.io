<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Alex Zhao - markdown</title><link href="https://www.alex-zhao.com/" rel="alternate"></link><link href="https://www.alex-zhao.com/feeds/markdown.atom.xml" rel="self"></link><id>https://www.alex-zhao.com/</id><updated>2018-11-14T00:00:00-05:00</updated><entry><title>Don't p-hack: Three Lessons for Data Science in a Public Relations Setting</title><link href="https://www.alex-zhao.com/2018/11/14/dont-p-hack/" rel="alternate"></link><published>2018-11-14T00:00:00-05:00</published><updated>2018-11-14T00:00:00-05:00</updated><author><name>Alex Zhao</name></author><id>tag:www.alex-zhao.com,2018-11-14:/2018/11/14/dont-p-hack/</id><summary type="html">&lt;p&gt;I used to work as a data analyst on the marketing team at &lt;a href="https://www.grubhub.com/"&gt;GrubHub&lt;/a&gt;, an online food delivery company. During this time, some of the more interesting data science tasks I was given included trying to find insights for our public relations team. Some of these were based on current …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I used to work as a data analyst on the marketing team at &lt;a href="https://www.grubhub.com/"&gt;GrubHub&lt;/a&gt;, an online food delivery company. During this time, some of the more interesting data science tasks I was given included trying to find insights for our public relations team. Some of these were based on current events, like for this article about &lt;a href="https://www.huffingtonpost.com/2012/11/07/election-night-food-delivery-grubhub_n_2090161.html"&gt;food orders during the 2012 election&lt;/a&gt;. Others were more self-directed, like this analysis a colleague did for NPR about &lt;a href="https://www.npr.org/sections/money/2014/02/26/282132576/74-476-reasons-you-should-always-get-the-bigger-pizza"&gt;pizza size and value&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I thought of this because of an article I read earlier this year about &lt;a href="https://www.buzzfeednews.com/article/stephaniemlee/brian-wansink-cornell-p-hacking#.bmxLG1XPpN"&gt;Brian Wansik&lt;/a&gt;, a Cornell professor whose work garnered widespread media attention over the years before it was found that he had committed scientific misconduct. Specifically, Wansik "p-hacked," manipulating his data and analysis until he got a p-value below 0.05, the generally accepted threshold for &lt;a href="https://en.wikipedia.org/wiki/Statistical_significance"&gt;statistical significance&lt;/a&gt;. And while academic and public relations are very different fields, what Wansik did wrong can still help to guide data science for public relations.&lt;/p&gt;
&lt;h3&gt;Lesson One: Don't p-hack&lt;/h3&gt;
&lt;p&gt;This is the most immediate takeaway from Wansik's mistakes. It should be obvious that trying to manipulate your data and analysis to get a significant result is a bad idea. To be clear, this doesn't preclude subsetting or otherwise adjusting your approaching during the course of your analysis, but it does mean that once you get a bad result, you can't just keep changing things until you get a good result.&lt;/p&gt;
&lt;p&gt;At a conceptual level, it might be difficult to tell the difference between p-hacking and legitimate adjustments of data or analyses in order to correct for mistakes or other problems with what you have done. In general, though, if you are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Precommitting to a hypothesis beforehand (that is, before you get the data or start diving into it), and&lt;/li&gt;
&lt;li&gt;You have reasonable explanations or hypotheses for why you are adjusting your data approach (for example if you believe that there is a seasonality effect), and&lt;/li&gt;
&lt;li&gt;You are adding, not removing information (adding more variables or columns), with the exceptions of,&lt;/li&gt;
&lt;li&gt;a. Adding more to your sample size indefinitely (which will inevitably get you a significant p-value), or
   b. Removing outliers&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Then what you're doing will tend to stick to the right side of the legitimate adjustment/p-hacking line.&lt;/p&gt;
&lt;h3&gt;Lesson Two: Precommit to hypotheses (and some other stuff) beforehand&lt;/h3&gt;
&lt;p&gt;As highlighted in the guidelines above, and as a general good practice, you should commit to what hypothesis you want to test beforehand. If you believe that you have a question that people would find interesting, clearly articulate what it is before you start doing analysis on the data. If you're collecting the data yourself, you need to make sure you specify how many observations you'll be collecting as well as what information you want. This avoids two common problems that can lead to bad data science. First, it prevents the scenario where you are blindly digging around the data just to find something interesting (which leads to p-hacking). Second, it prevents retroactively creating a hypothesis based on data you've already looked at. This kind of retroactive hypothesis is especially bad because it defeats the entire purpose of doing data science. Creating a retroactive hypothesis is like looking up spoilers for a movie before seeing it and then making a prediction about how it ends: it is no great feat of analysis to "predict" an answer you already know, and using the data to generate your hypotheses retroactively means that your analysis is not particularly helpful or useful.&lt;/p&gt;
&lt;h3&gt;Lesson Three: If not testing, list out your methodology beforehand&lt;/h3&gt;
&lt;p&gt;Not all data science for public relations involves answering questions with a kind of yes or no answer like the one about whether food orders increased on election night. The staple of any kind of analysis done for PR is usually the list, where a company either releases the "most popular X in country Y" or "the most popular Z by state/province" or "the cities with the most (blank)."  There's nothing wrong with this kind of work, but two good rules to follow include:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Making your sure explicitly define what metric you're ranking by, and&lt;/li&gt;
&lt;li&gt;Making sure you specify any kinds of adjustments you'd do to the rankings to make it not just dominated by the most popular answer&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The second rule is mostly done because places with higher populations will just generally have more of something, and it is extremely boring if every ranking just had say New York first (sometimes people want to see if something is disproportionately popular in a place, for example). In general, try to avoid constantly remaking your rankings just because you think that the results are not to your liking. This is a good rule of thumb whenever you do data science, but especially so when you're dealing with PR work, because such work is prone to suffering from this problem.&lt;/p&gt;
&lt;h3&gt;Bottom Line&lt;/h3&gt;
&lt;p&gt;While there might be some edge cases here and there when dealing with this kind of work, in general these lessons are good guidelines for making sure that your data science work for PR purposes is both interesting and analytically sound. In short, if you&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;State out what you want to test or how you're ranking before you start doing the analysis&lt;/li&gt;
&lt;li&gt;Don't constantly readjust your analysis just because the results are not to your liking&lt;/li&gt;
&lt;li&gt;Don't try to cut up your data by subsetting in order to "salvage" a result&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;You should be able to produce insightful data science work that both is generally sound and will give your marketing teams something worthwhile to generate PR.&lt;/p&gt;</content><category term="statistics"></category><category term="business"></category><category term="marketing"></category><category term="public relations"></category><category term="pr"></category></entry><entry><title>The Unroll.me Data That Uber Bought Probably Isn't That Anonymous</title><link href="https://www.alex-zhao.com/2017/04/25/uber-unrollme-not-anon/" rel="alternate"></link><published>2017-04-25T00:00:00-04:00</published><updated>2017-04-25T00:00:00-04:00</updated><author><name>Alex Zhao</name></author><id>tag:www.alex-zhao.com,2017-04-25:/2017/04/25/uber-unrollme-not-anon/</id><summary type="html">&lt;p&gt;As was to be expected, a recent &lt;a href="https://www.nytimes.com/2017/04/23/technology/travis-kalanick-pushes-uber-and-himself-to-the-precipice.html"&gt;New York Times article about Uber&lt;/a&gt; has gotten the internet into an uproar over how Uber conducts its business. What's strange, however, is that while much of the internet's displeasure has been focused on Uber (and reading the article points out several aggressive …&lt;/p&gt;</summary><content type="html">&lt;p&gt;As was to be expected, a recent &lt;a href="https://www.nytimes.com/2017/04/23/technology/travis-kalanick-pushes-uber-and-himself-to-the-precipice.html"&gt;New York Times article about Uber&lt;/a&gt; has gotten the internet into an uproar over how Uber conducts its business. What's strange, however, is that while much of the internet's displeasure has been focused on Uber (and reading the article points out several aggressive and probably borderline unethical actions by the company), the biggest casualty from this story might not be Uber at all. Instead, another company, Slice Technologies, owner of a popular inbox decluttering service &lt;a href="https://unroll.me/"&gt;Unroll.me&lt;/a&gt;, was revealed to have &lt;a href="https://www.nytimes.com/2017/04/24/technology/personal-data-firm-slice-unroll-me-backlash-uber.html"&gt;sold receipt data&lt;/a&gt; from Uber's competitor Lyft to Uber, in a supposedly anonymized fashion.&lt;/p&gt;
&lt;p&gt;I personally don't think that Slice's particular defenses (we told you we do this and everyone else does it) are particularly compelling. The former depends on a Terms of Service no human being is actually reading, and the latter isn't quite true: the closest analogy I have seen would be Google mining its own data (in its apps and services) to monitor employees of competitors like Apple and Facebook. But that is an issue for another day. &lt;/p&gt;
&lt;p&gt;My problem with this whole saga is the fact that &lt;strong&gt;the information Slice sold really isn't that anonymous&lt;/strong&gt;.&lt;/p&gt;
&lt;h3&gt;Location data might be enough&lt;/h3&gt;
&lt;p&gt;Let's suppose for a second that Slice did what it thought was due diligence and removed the names from every receipt. From the articles that I've read, that is essentially the extent to which the data was anonymized and sold to Uber. &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Unroll.me, a free service to unsubscribe from email lists, can scour people’s inboxes for receipts from services like Lyft and then sell the information to companies like Uber. The data is anonymized, meaning individuals’ names are not attached to the information, and can be used as a proxy for the health of a rival.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It's possible that Slice also protected some of the location data, but a) that's very much unclear from its own &lt;a href="https://www.slice.com/privacy"&gt;privacy policy&lt;/a&gt;, and b) If they sold receipt-level data, it's possible both that Uber wanted that location data to keep track of Lyft and that the data is there anyways. At the very least, nothing in Slice's privacy policy prohibits this, and it's very likely they make as much of the individual receipt-level information available as possible.&lt;/p&gt;
&lt;p&gt;This is a problem, since a Lyft receipt looks like this:
&lt;img alt="Lyft Receipt" src="https://help.lyft.com/hc/en-us/article_attachments/204568237/unnamed.png" title="Lyft Receipt"&gt;&lt;/p&gt;
&lt;p&gt;If the reporting around these data sales is true, then what we assume is that all of the information in the receipt above, sans the customer's name, were sold to Uber. That means the last 4 of the credit card, the pickup, and the dropoff, would all be information that Uber could effectively parse. In the abstract, missing the names might be an effective method for eliminating personally identifiable information that would allow someone to trace individual trips back to an actual person. That is to say, this it's difficult to take that kind of receipt data and expose individual identities from just that database alone.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The problem is that Uber has another pretty much identical database which can be used to reveal these receipt-level identities.&lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;How to take "anonymous" data and find identities if you're Uber&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Take your own users' data, specifically any saved addresses (for example for Home or Work)&lt;/li&gt;
&lt;li&gt;In addition to this, find for each user their most commonly used addresses/GPS coordinate region (say, within 50 feet) both for pickup and dropoff&lt;/li&gt;
&lt;li&gt;Since the Slice data exists also at the user level, do the same thing for receipts you get for each distinct customer&lt;/li&gt;
&lt;li&gt;If Slice gives you the last 4 of the credit cards for each receipts, compare those with your internal data as well&lt;/li&gt;
&lt;li&gt;Use the information from steps 1-4 (and information about when rides were requested as well) to match up users&lt;/li&gt;
&lt;li&gt;Now you have information on which of your Uber riders use Lyft, and their names.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There are, of course, many caveats to this process. For starters, it's not very well defined since the objective here was not to fully flesh out the exact process by which to match up this supposedly anonymous data with Uber's own data, but rather to point out the general direction by which it could be done. It's also possible that Slice Technologies is better about data privacy than the articles claim, since they might simply aggregate data up at a level that makes it harder to tell who individual users are. Moreover, it's possible that Slice neither provided nor Uber requested that level of granularity, though I am more skeptical about the latter case and nothing within Slice's privacy policy actually rules that out (and indeed reserves the right to sell email messages). And finally, even given all this information, you probably can't get a 100% match between Lyft and Uber users (though very good rates are not out of the question).&lt;/p&gt;
&lt;h4&gt;Do a better job of protecting user privacy&lt;/h4&gt;
&lt;p&gt;Nonetheless, the biggest problem was simply the claim that removing people's names from this receipt level data was sufficient to anonymize the information. Perhaps in the abstract this is true, but when everything else about the receipt information is available, it's not that hard to figure out a name. Indeed, Uber would be far from the only company that could do this: any e-commerce business that stores addresses or tracks locations could use this level of data to find the identities of Lyft users. Given all of the potential touchpoints available from these receipts (and other touchpoints Slice has, including but not limited to device IDs, geographical location, times of purchase, amount of purchase, and last 4 of the credit card), to call this data anonymous simply because names were removed is simply wrong.&lt;/p&gt;
&lt;p&gt;Data and databases don't exist simply in the abstract. When considering how to securely release or anonymize your data, not accounting for the data that the party you're selling to might have is one surefire way to leak data you didn't want to reveal in the first place. At the very least, companies should be revealing as little data as possible to third parties if they truly care about protecting user privacy. Barring that, aggregating the data or adding random noise to the user-level data is imperative if you actually care about obscuring personally identifiable information.&lt;/p&gt;
&lt;p&gt;Also, if you haven't already, you should probably delete your Unroll.me account.&lt;/p&gt;</content><category term="technology"></category><category term="startups"></category><category term="data privacy"></category></entry></feed>